#!/usr/bin/env python3
"""
–ò–º–ø–æ—Ä—Ç –≥—Ä—É–ø–ø –∏–∑ targets.txt –∏ group_niches.json –≤ PostgreSQL –ë–î
"""
import sys
import json
from pathlib import Path
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent.parent))

from shared.database.session import SessionLocal, init_db
from shared.database.models import Group
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def import_groups(base_dir: Path, niche: str = 'cars'):
    """
    –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –≥—Ä—É–ø–ø—ã –∏–∑ targets.txt –∏ group_niches.json
    
    Args:
        base_dir: –ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø—Ä–æ–µ–∫—Ç–∞
        niche: –ù–∏—à–∞ –¥–ª—è –≥—Ä—É–ø–ø (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'cars')
    """
    logger.info("=" * 80)
    logger.info(f"üì• IMPORTING GROUPS (niche: {niche})")
    logger.info("=" * 80)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î
    try:
        init_db()
        logger.info("‚úÖ Database initialized")
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize database: {e}")
        return
    
    targets_file = base_dir / "targets.txt"
    niches_file = base_dir / "group_niches.json"
    
    # –ß–∏—Ç–∞–µ–º targets.txt
    groups_list = []
    if targets_file.exists():
        logger.info(f"üìÑ Reading {targets_file}")
        with open(targets_file, 'r', encoding='utf-8') as f:
            for line in f:
                username = line.strip()
                if username and username.startswith('@'):
                    groups_list.append(username)
        logger.info(f"Found {len(groups_list)} groups in targets.txt")
    else:
        logger.warning(f"‚ö†Ô∏è {targets_file} not found")
        return
    
    # –ß–∏—Ç–∞–µ–º group_niches.json
    niches_map = {}
    if niches_file.exists():
        logger.info(f"üìÑ Reading {niches_file}")
        with open(niches_file, 'r', encoding='utf-8') as f:
            niches_map = json.load(f)
        logger.info(f"Found {len(niches_map)} group niches")
    else:
        logger.warning(f"‚ö†Ô∏è {niches_file} not found, using default niche for all groups")
    
    db = SessionLocal()
    try:
        imported = 0
        updated = 0
        skipped = 0
        
        for username in groups_list:
            try:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∏—à—É –¥–ª—è –≥—Ä—É–ø–ø—ã
                group_niche = niches_map.get(username, niche)
                
                # –ï—Å–ª–∏ –Ω–∏—à–∞ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±—É–µ–º–æ–π - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (–∏–ª–∏ –º–µ–Ω—è–µ–º –Ω–∏—à—É)
                if niche != 'all' and group_niche != niche:
                    skipped += 1
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–∞–∫–∞—è –≥—Ä—É–ø–ø–∞
                existing = db.query(Group).filter(Group.username == username).first()
                
                if existing:
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –≥—Ä—É–ø–ø—É
                    existing.niche = group_niche
                    existing.status = 'active'  # –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º –≥—Ä—É–ø–ø—É
                    updated += 1
                else:
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –≥—Ä—É–ø–ø—É
                    new_group = Group(
                        username=username,
                        niche=group_niche,
                        status='active',
                        can_post=True
                    )
                    db.add(new_group)
                    imported += 1
                
                # –ö–æ–º–º–∏—Ç–∏–º –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã (–¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏)
                db.commit()
                
            except Exception as e:
                db.rollback()
                logger.error(f"‚ùå Error processing {username}: {e}")
                skipped += 1
        
        logger.info("=" * 80)
        logger.info(f"‚úÖ Import completed:")
        logger.info(f"   - Imported: {imported}")
        logger.info(f"   - Updated: {updated}")
        logger.info(f"   - Skipped: {skipped}")
        logger.info(f"   - Total processed: {len(groups_list)}")
        
    except Exception as e:
        db.rollback()
        logger.error(f"‚ùå Error during import: {e}")
    finally:
        db.close()


def import_assignments(base_dir: Path):
    """
    –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –ø—Ä–∏–≤—è–∑–∫–∏ –∞–∫–∫–∞—É–Ω—Ç–æ–≤ –∫ –≥—Ä—É–ø–ø–∞–º –∏–∑ group_account_assignments.json (–µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
    """
    assignments_file = base_dir / "group_account_assignments.json"
    
    if not assignments_file.exists():
        logger.info("‚ÑπÔ∏è group_account_assignments.json not found, skipping assignments import")
        return
    
    logger.info(f"üìÑ Reading assignments from {assignments_file}")
    
    with open(assignments_file, 'r', encoding='utf-8') as f:
        assignments = json.load(f)
    
    db = SessionLocal()
    try:
        from shared.database.models import Account
        
        updated = 0
        
        for username, data in assignments.items():
            try:
                # –ù–∞—Ö–æ–¥–∏–º –≥—Ä—É–ø–ø—É
                group = db.query(Group).filter(Group.username == username).first()
                if not group:
                    logger.warning(f"‚ö†Ô∏è Group {username} not found in DB, skipping")
                    continue
                
                # –ù–∞—Ö–æ–¥–∏–º –∞–∫–∫–∞—É–Ω—Ç
                account_name = data.get('account')
                if not account_name:
                    continue
                
                account = db.query(Account).filter(Account.session_name == account_name).first()
                if not account:
                    logger.warning(f"‚ö†Ô∏è Account {account_name} not found in DB, skipping")
                    continue
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–∏–≤—è–∑–∫—É
                group.assigned_account_id = account.id
                
                # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—ã
                if data.get('joined_at'):
                    try:
                        group.joined_at = datetime.fromisoformat(data['joined_at'].replace('Z', '+00:00'))
                    except:
                        pass
                
                if data.get('warm_up_until'):
                    try:
                        group.warm_up_until = datetime.fromisoformat(data['warm_up_until'].replace('Z', '+00:00'))
                    except:
                        pass
                
                db.commit()
                updated += 1
                
            except Exception as e:
                db.rollback()
                logger.error(f"‚ùå Error processing assignment for {username}: {e}")
        
        logger.info(f"‚úÖ Updated {updated} group assignments")
        
    except Exception as e:
        db.rollback()
        logger.error(f"‚ùå Error importing assignments: {e}")
    finally:
        db.close()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Import groups from targets.txt')
    parser.add_argument('--niche', default='cars', help='Niche to import (default: cars)')
    parser.add_argument('--all', action='store_true', help='Import all niches')
    args = parser.parse_args()
    
    base_dir = Path(__file__).parent.parent
    
    niche = 'all' if args.all else args.niche
    import_groups(base_dir, niche=niche)
    
    # –ü—ã—Ç–∞–µ–º—Å—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–≤—è–∑–∫–∏
    import_assignments(base_dir)

